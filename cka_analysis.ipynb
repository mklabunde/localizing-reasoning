{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0305d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def prepare_dataframe(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"canonical_pair\"] = df.apply(lambda x: \"__\".join(sorted([x[\"model_1\"], x[\"model_2\"]])), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def show_linearcka_to_corresponding_layer(df: pd.DataFrame):\n",
    "    for base_model in set(df.model_1.unique()).union(set(df.model_2.unique())):\n",
    "        data = df[(df.model_1 == base_model) | (df.model_2 == base_model)].copy()\n",
    "\n",
    "        # data = data[(data.layer_1 == data.layer_2)]\n",
    "        layer_to_cat = {0: 0, 6: 6, 12: 12, 18: 18, 35: \"final\", 27: \"final\"}\n",
    "        cat_order = [0, 6, 12, 18, \"final\"]\n",
    "        data[\"depth_cat_1\"] = pd.Categorical(data[\"layer_1\"].map(layer_to_cat), categories=cat_order, ordered=True)\n",
    "        data[\"depth_cat_2\"] = pd.Categorical(data[\"layer_2\"].map(layer_to_cat), categories=cat_order, ordered=True)\n",
    "        data = data[(data.depth_cat_1 == data.depth_cat_2)]\n",
    "\n",
    "        data[\"descendant\"] = data[\"canonical_pair\"].apply(lambda s: re.sub(base_model, \"\", s, count=1).strip(\"_\"))\n",
    "        data\n",
    "\n",
    "        g = sns.catplot(data, x=\"depth_cat_1\", y=\"cka_linear\", hue=\"descendant\", kind=\"point\", col=\"token_type\")\n",
    "        g.axes[0, 0].set_ylabel(f\"cka_linear to {base_model} at same layer\")\n",
    "        # sns.catplot(data, x=\"layer_1\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"point\", col=\"token_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807141c",
   "metadata": {},
   "source": [
    "## Models descending from Qwen-R1-distill-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00bf33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_dataframe(\"cka_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46707d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = df.pivot_table(values=\"cka_linear\", index=[\"token_type\", \"layer_1\"], columns=[\"canonical_pair\", \"layer_2\"])\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.canonical_pair.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbd03b",
   "metadata": {},
   "source": [
    "All models descend from DeepSeek-R1-Distill-Qwen-7B. How did they change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c79bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"DeepSeek-R1-Distill-Qwen-7B\"\n",
    "data = df[(df.model_1 == base_model) | (df.model_2 == base_model)].copy()\n",
    "\n",
    "\n",
    "# data = data[(data.layer_1 == data.layer_2)]\n",
    "layer_to_cat = {0: 0, 6: 6, 12: 12, 18: 18, 35: \"final\", 27: \"final\"}\n",
    "cat_order = [0, 6, 12, 18, \"final\"]\n",
    "data[\"depth_cat_1\"] = pd.Categorical(data[\"layer_1\"].map(layer_to_cat), categories=cat_order, ordered=True)\n",
    "data[\"depth_cat_2\"] = pd.Categorical(data[\"layer_2\"].map(layer_to_cat), categories=cat_order, ordered=True)\n",
    "data = data[(data.depth_cat_1 == data.depth_cat_2)]\n",
    "\n",
    "data[\"descendant\"] = data[\"canonical_pair\"].apply(lambda s: re.sub(base_model, \"\", s, count=1).strip(\"_\"))\n",
    "data\n",
    "\n",
    "g = sns.catplot(data, x=\"depth_cat_1\", y=\"cka_linear\", hue=\"descendant\", kind=\"point\", col=\"token_type\")\n",
    "g.axes[0, 0].set_ylabel(f\"cka_linear to {base_model} at same layer\")\n",
    "# sns.catplot(data, x=\"layer_1\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"point\", col=\"token_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20721b",
   "metadata": {},
   "source": [
    "Only Light-R1 is a pure SFT model, the others also involve some RL. Does the SFT model have different representations than the RL(+SFT) models?\n",
    "\n",
    "We are checking pairwise similarities now to test the hypothesis that SFT has a different effect than RL on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd534552",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_linearcka_to_corresponding_layer(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8e715",
   "metadata": {},
   "source": [
    "### With same input tokens\n",
    "\n",
    "Comparing reps of all tokens between corresponding layers\n",
    "\n",
    "Caveat: answers longer than 8096 tokens were cut off to avoid OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_dataframe(\"cka_fixed_input_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e742448",
   "metadata": {},
   "source": [
    "Filtering out the final layer, because it is way lower than the rest. \n",
    "Makes sense, because their next token predictions are probably different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = df.loc[df.layer <= 21]\n",
    "# sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", style=\"token_origin\")\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", style=\"token_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = df.loc[df.layer < df.layer.max()]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", style=\"token_origin\")\n",
    "\n",
    "plotdata = df[df.layer < df.layer.max()]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", style=\"token_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = df  # .loc[df.layer < df.layer.max()]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", style=\"token_origin\")\n",
    "\n",
    "plotdata = df  # [df.layer < df.layer.max()]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", style=\"token_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = df.loc[df.layer < df.layer.max()]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df[df.layer < df.layer.max()]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d95997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99065dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"contains_basemodel\"] = (df[\"model_1\"] == \"DeepSeek-R1-Distill-Qwen-7B\") | (\n",
    "    df[\"model_2\"] == \"DeepSeek-R1-Distill-Qwen-7B\"\n",
    ")\n",
    "\n",
    "plotdata = df.loc[df[\"contains_basemodel\"] & (df.layer < df.layer.max())]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df.loc[df[\"contains_basemodel\"] & (df.layer < df.layer.max())]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499686f4",
   "metadata": {},
   "source": [
    "## Models descending from Qwen2.5-Math-7B (optionally +rope extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_dataframe(\"cka_results_qwen2.5-math.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857675bd",
   "metadata": {},
   "source": [
    "openr1-distill and qwen-r1-distill are pure SFT models, acereason1.1 also involves some RL. Does the SFT model have different representations than the RL(+SFT) models?\n",
    "\n",
    "We are checking pairwise similarities now to test the hypothesis that SFT has a different effect than RL on the model.\n",
    "\n",
    "openr1 is a descendant of qwen2.5-math-rope300k. the rope300k model should be basically identical to the regular qwen2.5 math model. Lets check\n",
    "\n",
    "\n",
    "**Result:**\\\n",
    "qwen-rope is quite different to qwen.\\\n",
    "acereason and openr1 are more similar to each other compared to the base cot models\\\n",
    "during reasoning, all models are dissimilar (but that could be confounded)\n",
    "\n",
    "The reasoning step analysis is not robust enough: the idea was that models that reason in roughly the same direction have similar representations. But their reasoning direction can change all the time with _wait_ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_linearcka_to_corresponding_layer(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b862c1fe",
   "metadata": {},
   "source": [
    "### Same analysis as above with fixed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_dataframe(\"cka_fixed_input_results_qwen2.5-math-extra.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotdata = df.loc[df.layer < df.layer.max()]\n",
    "plotdata = df\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "# plotdata = df[df.layer < df.layer.max()]\n",
    "plotdata = df\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"contains_basemodel\"] = (df[\"model_1\"] == \"Qwen2.5-Math-7B\") | (df[\"model_2\"] == \"Qwen2.5-Math-7B\")\n",
    "\n",
    "plotdata = df.loc[df[\"contains_basemodel\"]]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df[df[\"contains_basemodel\"]]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16793ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"contains_basemodel\"] = (df[\"model_1\"] == \"Qwen2.5-Math-7B\") | (df[\"model_2\"] == \"Qwen2.5-Math-7B\")\n",
    "\n",
    "plotdata = df.loc[df[\"contains_basemodel\"] & (df.layer < df.layer.max())]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df[df[\"contains_basemodel\"] & (df.layer < df.layer.max())]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d642c",
   "metadata": {},
   "source": [
    "## Models descending from R1-distill-Qwen2.5-1.5B\n",
    "\n",
    "only with fixed tokens, generated from the qwen25-math descendants or the r1-qwen-7b descendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_dataframe(\"cka_fixed_input_results_r1-distill-qwen2.5-1.5b-1-cache2.csv\")\n",
    "df[\"contains_basemodel\"] = (df[\"model_1\"] == \"DeepSeek-R1-Distill-Qwen-1-5B\") | (\n",
    "    df[\"model_2\"] == \"DeepSeek-R1-Distill-Qwen-1-5B\"\n",
    ")\n",
    "\n",
    "# plotdata = df.loc[df.layer < df.layer.max()]\n",
    "plotdata = df\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "# plotdata = df[df.layer < df.layer.max()]\n",
    "plotdata = df\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df.loc[df[\"contains_basemodel\"]]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df[df[\"contains_basemodel\"]]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075507bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_dataframe(\"cka_fixed_input_results_r1-distill-qwen2.5-1.5b-1-cache1.csv\")\n",
    "df[\"contains_basemodel\"] = (df[\"model_1\"] == \"DeepSeek-R1-Distill-Qwen-1-5B\") | (\n",
    "    df[\"model_2\"] == \"DeepSeek-R1-Distill-Qwen-1-5B\"\n",
    ")\n",
    "\n",
    "# plotdata = df.loc[df.layer < df.layer.max()]\n",
    "plotdata = df\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "# plotdata = df[df.layer < df.layer.max()]\n",
    "plotdata = df\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df.loc[df[\"contains_basemodel\"]]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df[df[\"contains_basemodel\"]]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc50b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_dataframe(\"cka_fixed_input_results_r1-distill-qwen2.5-1.5b-2-cache2.csv\")\n",
    "df[\"contains_basemodel\"] = (df[\"model_1\"] == \"DeepSeek-R1-Distill-Qwen-1-5B\") | (\n",
    "    df[\"model_2\"] == \"DeepSeek-R1-Distill-Qwen-1-5B\"\n",
    ")\n",
    "\n",
    "# plotdata = df.loc[df.layer < df.layer.max()]\n",
    "plotdata = df\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "# plotdata = df[df.layer < df.layer.max()]\n",
    "plotdata = df\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df.loc[df[\"contains_basemodel\"]]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df[df[\"contains_basemodel\"]]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_dataframe(\"cka_fixed_input_results_r1-distill-qwen2.5-1.5b-2-cache1.csv\")\n",
    "df[\"contains_basemodel\"] = (df[\"model_1\"] == \"DeepSeek-R1-Distill-Qwen-1-5B\") | (\n",
    "    df[\"model_2\"] == \"DeepSeek-R1-Distill-Qwen-1-5B\"\n",
    ")\n",
    "\n",
    "# plotdata = df.loc[df.layer < df.layer.max()]\n",
    "plotdata = df\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "# plotdata = df[df.layer < df.layer.max()]\n",
    "plotdata = df\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df.loc[df[\"contains_basemodel\"]]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")\n",
    "\n",
    "plotdata = df[df[\"contains_basemodel\"]]\n",
    "sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"canonical_pair\", kind=\"line\", col=\"token_origin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caed852",
   "metadata": {},
   "source": [
    "### combined view across caches (responses) and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "base_model = \"DeepSeek-R1-Distill-Qwen-1-5B\"\n",
    "dfs = [\n",
    "    prepare_dataframe(\"cka_fixed_input_results_r1-distill-qwen2.5-1.5b-1-cache2.csv\"),\n",
    "    prepare_dataframe(\"cka_fixed_input_results_r1-distill-qwen2.5-1.5b-1-cache1.csv\"),\n",
    "    prepare_dataframe(\"cka_fixed_input_results_r1-distill-qwen2.5-1.5b-2-cache2.csv\"),\n",
    "    prepare_dataframe(\"cka_fixed_input_results_r1-distill-qwen2.5-1.5b-2-cache1.csv\"),\n",
    "]\n",
    "df = pd.concat(dfs, axis=0)\n",
    "df[\"base_model\"] = base_model\n",
    "all_data.append(df)\n",
    "\n",
    "base_model = \"Qwen2.5-Math-7B\"\n",
    "df = prepare_dataframe(\"cka_fixed_input_results_qwen2.5-math-extra.csv\")\n",
    "df[\"base_model\"] = base_model\n",
    "all_data.append(df)\n",
    "\n",
    "base_model = \"DeepSeek-R1-Distill-Qwen-7B\"\n",
    "dfs = [\n",
    "    prepare_dataframe(\"cka_fixed_input_results.csv\"),\n",
    "    prepare_dataframe(\"cka_fixed_input_results_cache2.csv\"),\n",
    "]\n",
    "df = pd.concat(dfs, axis=0)\n",
    "df[\"base_model\"] = base_model\n",
    "all_data.append(df)\n",
    "\n",
    "df = pd.concat(all_data, axis=0).reset_index()\n",
    "df[\"contains_basemodel\"] = (df[\"model_1\"] == df[\"base_model\"]) | (df[\"model_2\"] == df[\"base_model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb821a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = df.loc[df[\"contains_basemodel\"]].copy()\n",
    "plotdata = plotdata[plotdata.layer < plotdata.layer.max()]\n",
    "plotdata[\"descendant\"] = plotdata.apply(\n",
    "    lambda x: x[\"model_1\"] if x[\"model_2\"] == x[\"base_model\"] else x[\"model_2\"], axis=1\n",
    ")\n",
    "\n",
    "sns.catplot(\n",
    "    plotdata,\n",
    "    x=\"layer\",\n",
    "    y=\"cka_linear\",\n",
    "    hue=\"descendant\",\n",
    "    col=\"token_origin\",\n",
    "    sharey=False,\n",
    "    row=\"base_model\",\n",
    "    kind=\"point\",\n",
    ")\n",
    "# sns.relplot(plotdata, x=\"layer\", y=\"cka_linear\", hue=\"descendant\", kind=\"line\", col=\"token_origin\")\n",
    "# sns.relplot(plotdata, x=\"layer\", y=\"cka_rbf\", hue=\"descendant\", kind=\"line\", col=\"token_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = (\n",
    "    plotdata.groupby([\"base_model\", \"descendant\", \"token_origin\", \"layer\"])[\"cka_linear\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef97fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    pivot[(pivot.base_model == \"DeepSeek-R1-Distill-Qwen-1-5B\")],\n",
    "    # pivot[(pivot.base_model == \"DeepSeek-R1-Distill-Qwen-1-5B\") & (pivot[\"mean\"] > 0.99)],\n",
    "    x=\"layer\",\n",
    "    y=\"mean\",\n",
    "    hue=\"descendant\",\n",
    "    kind=\"line\",\n",
    "    col=\"token_origin\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot[\"descendant\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6dfed3",
   "metadata": {},
   "source": [
    "## In-depth analysis of cka scores\n",
    "\n",
    "Lets look at a model and dataset, where we see the bathtub shape.\n",
    "\n",
    "Then lets look at:\n",
    "* grad of score wrt kernel elements (which elements have the biggest influence on the similarity score)\n",
    "* stats about values in the kernel matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [\n",
    "    {\n",
    "        \"token_origin\": \"DeepSeek-R1-Distill-Qwen-7B\",\n",
    "        \"model\": \"Qwen2.5-Math-7B-Oat-Zero\",\n",
    "        \"model_hf\": \"sail/Qwen2.5-Math-7B-Oat-Zero\",\n",
    "        \"base_model\": \"Qwen2.5-Math-7B\",\n",
    "        \"base_model_hf\": \"Qwen/Qwen2.5-Math-7B\",\n",
    "        \"bathtub\": \"middle\",\n",
    "    },\n",
    "    {\n",
    "        \"token_origin\": \"DeepSeek-R1-Distill-Qwen-7B\",\n",
    "        # \"model\": \"OpenR1-Distill\",\n",
    "        # \"model_hf\": \"open-r1/OpenR1-Distill-7B\",\n",
    "        \"model\": \"AceReason-Nemotron-1.1-7B\",\n",
    "        \"model_hf\": \"nvidia/AceReason-Nemotron-1.1-7B\",\n",
    "        \"base_model\": \"Qwen2.5-Math-7B\",\n",
    "        \"base_model_hf\": \"Qwen/Qwen2.5-Math-7B\",\n",
    "        \"bathtub\": \"everywhere\",\n",
    "    },\n",
    "    {\n",
    "        \"token_origin\": \"DeepSeek-R1-Distill-Qwen-7B\",\n",
    "        \"model\": \"Nemotron-Research-Reasoning-Qwen-1-5B\",\n",
    "        \"base_model\": \"DeepSeek-R1-Distill-Qwen-1-5B\",\n",
    "        \"bathtub\": \"nowhere\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be299d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = settings[0]\n",
    "\n",
    "subdf = df.loc[\n",
    "    (df[\"base_model\"] == setting[\"base_model\"])\n",
    "    & ((df[\"model_1\"] == setting[\"model\"]) | (df[\"model_2\"] == setting[\"model\"]))\n",
    "    & ((df[\"model_1\"] == setting[\"base_model\"]) | (df[\"model_2\"] == setting[\"base_model\"]))\n",
    "    & (df[\"token_origin\"] == setting[\"token_origin\"])\n",
    "]\n",
    "\n",
    "sns.relplot(data=subdf, x=\"layer\", y=\"cka_linear\", hue=\"sample\", kind=\"line\", legend=False, alpha=0.3)\n",
    "# sns.catplot(data=subdf, x=\"layer\", y=\"cka_linear\", kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c55237",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgsim = subdf.groupby(\"sample\")[\"cka_linear\"].mean()\n",
    "\n",
    "print(\n",
    "    \"top3 and bottom3 avg linear cka\\n\",\n",
    "    avgsim[(avgsim > avgsim.quantile(0.95)) | (avgsim < avgsim.quantile(0.05))].index,\n",
    ")\n",
    "sns.relplot(\n",
    "    data=subdf[\n",
    "        subdf[\"sample\"].isin(avgsim[(avgsim > avgsim.quantile(0.95)) | (avgsim < avgsim.quantile(0.05))].index)\n",
    "    ],\n",
    "    x=\"layer\",\n",
    "    y=\"cka_linear\",\n",
    "    hue=\"sample\",\n",
    "    kind=\"line\",\n",
    "    legend=True,\n",
    "    alpha=0.3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toks von sample 14 aus token_origin folder in cache_representation2 laden\n",
    "import torch\n",
    "\n",
    "from reasoning.cache import load_tokens\n",
    "from reasoning.cka import cka, gram_linear\n",
    "from reasoning.generation import get_hidden_states, get_model_and_tokenizer\n",
    "\n",
    "# sample_id = 14  # low sim\n",
    "# sample_id = 19  # low sim\n",
    "# sample_id = 42  # low sim\n",
    "# sample_id = 27  # high sim\n",
    "sample_id = 31  # high sim\n",
    "# sample_id = 44  # high sim\n",
    "cache_dir = f\"cache_representations_2/{setting['token_origin']}/sample_{sample_id}\"\n",
    "tokens = load_tokens(cache_dir)\n",
    "print(tokens.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91030233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# durch model und base_model jagen um hidden states zu kriegen\n",
    "model_name_descendant = setting[\"model_hf\"]\n",
    "# model_name_descendant = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "model_name_base = setting[\"base_model_hf\"]\n",
    "\n",
    "# model_name_base = \"meta-llama/Llama-3.1-8B\"\n",
    "# model_name_descendant = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_name_base = \"meta-llama/Llama-3.2-3B\"\n",
    "# model_name_descendant = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_name_base = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_name_descendant = \"zztheaven/Llama-3.2-3B-Instruct-Open-R1-GRPO-2\"\n",
    "# model_descendant, tokenizer_descendant = get_model_and_tokenizer(\n",
    "#     model_id=model_name_descendant, device=\"cuda:0\", torch_dtype=torch.float16\n",
    "# )\n",
    "# model_base, tokenizer_base = get_model_and_tokenizer(\n",
    "#     model_id=model_name_base, device=\"cuda:2\", torch_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "\n",
    "model_descendant, tokenizer_descendant = get_model_and_tokenizer(model_id=model_name_descendant, device=\"cuda:5\")\n",
    "model_base, tokenizer_base = get_model_and_tokenizer(model_id=model_name_base, device=\"cuda:2\")\n",
    "model_base.requires_grad_(False)\n",
    "model_descendant.requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b040874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Math-7B\")\n",
    "# input_str = tokenizer.decode(tokens)\n",
    "# tokens = tokenizer_base(input_str, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "# tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tokenizer_descendant(input_str, return_tensors=\"pt\")[\"input_ids\"][0] == tokens).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66438445",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base.model.embed_tokens.weight.device\n",
    "model_base.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_base = get_hidden_states(model_base, tokens)\n",
    "h_descendant = get_hidden_states(model_descendant, tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae538397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c10895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "\n",
    "CREATE_FIGURE = False\n",
    "\n",
    "\n",
    "def pca_svd(data):\n",
    "    \"\"\"\n",
    "    Computes principal components and explained variance using the SVD method.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): A tensor of shape (N, D) where N is the number of samples\n",
    "                             and D is the number of features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - torch.Tensor: The principal components (right singular vectors), shape (D, D).\n",
    "            - torch.Tensor: The explained variance for each component.\n",
    "    \"\"\"\n",
    "    # 1. Center the data\n",
    "    mean = torch.mean(data, dim=0)\n",
    "    centered_data = data - mean\n",
    "\n",
    "    # 2. Perform SVD\n",
    "    # torch.linalg.svd returns U, S, and Vh (V transpose)\n",
    "    # The singular values are returned in descending order. [3]\n",
    "    _, S, Vh = torch.linalg.svd(centered_data)\n",
    "\n",
    "    # The principal components are the right singular vectors (V), which is Vh.T\n",
    "    principal_components = Vh.T\n",
    "\n",
    "    # 3. Calculate explained variance\n",
    "    # The eigenvalues of the covariance matrix are the square of the singular values\n",
    "    # divided by (n_samples - 1).\n",
    "    eigenvalues = (S**2) / (data.shape[0] - 1)\n",
    "    total_variance = torch.sum(eigenvalues)\n",
    "    explained_variance = eigenvalues / total_variance\n",
    "\n",
    "    return principal_components, explained_variance\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def pc_projection(data: torch.Tensor, principal_components: torch.Tensor, k: int):\n",
    "    if k < 1:\n",
    "        raise ValueError(f\"Need to project at least one dimension, but {k=}.\")\n",
    "    return data @ principal_components[:, :k]\n",
    "\n",
    "\n",
    "def add_to_data(grad, kernel, data, layer_idx, model_name):\n",
    "    data.append(\n",
    "        {\n",
    "            \"grad_min\": grad.min().item(),\n",
    "            \"grad_max\": grad.max().item(),\n",
    "            \"grad_mean\": grad.mean().item(),\n",
    "            \"grad_median\": grad.median().item(),\n",
    "            \"kernel_min\": kernel.min().item(),\n",
    "            \"kernel_max\": kernel.max().item(),\n",
    "            \"kernel_mean\": kernel.mean().item(),\n",
    "            \"kernel_median\": kernel.median().item(),\n",
    "            \"layer\": layer_idx,\n",
    "            \"model\": model_name,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_downsampled_heatmap(grad, ax, bottom_row: bool):\n",
    "    downsampler = torch.nn.MaxPool2d(16)\n",
    "    downsampled = downsampler(grad.unsqueeze(0).unsqueeze(0))\n",
    "    sns.heatmap(downsampled.squeeze(0).squeeze(0), ax=ax, cmap=\"viridis\")\n",
    "    if bottom_row:\n",
    "        ax.set_xlabel(\"Token Index\")\n",
    "        ax.set_ylabel(\"\")  # Remove y-axis label\n",
    "    else:\n",
    "        ax.set_title(f\"Layer {layer_idx}\")\n",
    "        ax.set_xlabel(\"\")  # Remove x-axis label for the top row\n",
    "        ax.set_ylabel(\"\")  # Remove y-axis label\n",
    "\n",
    "\n",
    "# --- Setup for Combined Plot ---\n",
    "# Determine the number of layers to plot (excluding the embedding layer)\n",
    "num_layers_to_plot = min(len(h_base), len(h_descendant)) - 1\n",
    "if CREATE_FIGURE and num_layers_to_plot > 0:\n",
    "    # Create a figure with subplots: 2 rows, one column per layer\n",
    "    # Adjust figsize for better readability; width scales with the number of layers.\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=num_layers_to_plot, figsize=(4 * num_layers_to_plot, 4 * 2 + 1))\n",
    "    # If there's only one layer, axes will be a 1D array, so we reshape it to 2D\n",
    "    if num_layers_to_plot == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "\n",
    "# kernel matrices erstellen\n",
    "# durch cka score backproppen\n",
    "data = []\n",
    "pc_stats = []\n",
    "projs = []\n",
    "\n",
    "cka_comparison_table = []\n",
    "correlation_table = []\n",
    "top_proj_tokens = []\n",
    "for layer_idx in tqdm(range(1, min(len(h_base), len(h_descendant)))):  # we start with 1 to skip the embedding layer\n",
    "    # these tensors have size (batch, tokens, dim). batch is 1 for us\n",
    "    cka_device = \"cuda:4\"\n",
    "    X = h_base[layer_idx].to(cka_device).squeeze(0).to(torch.float32)\n",
    "    Y = h_descendant[layer_idx].to(cka_device).squeeze(0).to(torch.float32)\n",
    "\n",
    "    X.requires_grad_(True)\n",
    "    Y.requires_grad_(True)\n",
    "\n",
    "    gram_x_linear = gram_linear(X)\n",
    "    gram_y_linear = gram_linear(Y)\n",
    "    gram_x_linear.retain_grad()\n",
    "    gram_y_linear.retain_grad()\n",
    "    linear_cka_score = cka(gram_x_linear, gram_y_linear)\n",
    "    linear_cka_score.backward()\n",
    "\n",
    "    # --- Column index for the plot ---\n",
    "    col_idx = layer_idx - 1\n",
    "\n",
    "    # --- Plotting Saliency Map for the Base Model (Row 0) ---\n",
    "    grad = gram_x_linear.grad.cpu()\n",
    "    kernel = gram_x_linear.cpu()\n",
    "    add_to_data(grad, kernel, data, layer_idx, model_name_base)\n",
    "    if CREATE_FIGURE:\n",
    "        add_downsampled_heatmap(grad, axes[0, col_idx], False)\n",
    "\n",
    "    grad = gram_y_linear.grad.cpu()\n",
    "    kernel = gram_y_linear.cpu()\n",
    "    add_to_data(grad, kernel, data, layer_idx, model_name_descendant)\n",
    "    if CREATE_FIGURE:\n",
    "        add_downsampled_heatmap(grad, axes[1, col_idx], True)\n",
    "\n",
    "    # --- Taking stats of explained variance by principal components\n",
    "    pc_x, expvar_x = pca_svd(X)\n",
    "    pc_y, expvar_y = pca_svd(Y)\n",
    "    pc_stats.append((expvar_x, expvar_y))\n",
    "    one_dim_proj_x = pc_projection(X, pc_x, 1)\n",
    "    one_dim_proj_y = pc_projection(Y, pc_y, 1)\n",
    "    projs.append((one_dim_proj_x, one_dim_proj_y))\n",
    "\n",
    "    # --- Testing effect of high-pc1-projecting samples on CKA by removing them.\n",
    "    k = 10\n",
    "    topk_x = one_dim_proj_x.squeeze(-1).topk(k)\n",
    "    topk_y = one_dim_proj_y.squeeze(-1).topk(k)\n",
    "\n",
    "    top_indices = torch.tensor(list(set(topk_x.indices.tolist()) | set(topk_y.indices.tolist())))\n",
    "    top_pc_projs_x = one_dim_proj_x[top_indices]\n",
    "    top_pc_projs_y = one_dim_proj_y[top_indices]\n",
    "\n",
    "    X_removed = X[~top_indices]\n",
    "    Y_removed = Y[~top_indices]\n",
    "    gram_x_linear = gram_linear(X_removed)\n",
    "    gram_y_linear = gram_linear(Y_removed)\n",
    "    linear_cka_score_wo_highpc_samples = cka(gram_x_linear, gram_y_linear)\n",
    "    cka_comparison_table.append((layer_idx, linear_cka_score.item(), linear_cka_score_wo_highpc_samples.item()))\n",
    "\n",
    "    # --- Testing correlation between pc1 projection magnitude and activation magnitude\n",
    "    corr_x = pearsonr(one_dim_proj_x.cpu().squeeze(-1), torch.linalg.norm(X, dim=1).detach().cpu())\n",
    "    corr_y = pearsonr(one_dim_proj_y.cpu().squeeze(-1), torch.linalg.norm(Y, dim=1).detach().cpu())\n",
    "    correlation_table.append((layer_idx, corr_x, corr_y))\n",
    "\n",
    "    # --- Inspecting tokens with highest pc1 projection\n",
    "    top_proj_tokens.append([tokenizer_base.decode(t) for t in tokens[top_indices]])\n",
    "\n",
    "\n",
    "if CREATE_FIGURE and num_layers_to_plot > 0:\n",
    "    # Set y-axis labels for the first column to act as row titles\n",
    "    axes[0, 0].set_ylabel(model_name_base, fontsize=12, weight=\"bold\")\n",
    "    axes[1, 0].set_ylabel(model_name_descendant, fontsize=12, weight=\"bold\")\n",
    "\n",
    "    # Add a single, overarching title for the entire figure\n",
    "    fig.suptitle(\n",
    "        f\"Saliency Maps of Gram Matrices per Layer (16x16 downsample, sample={sample_id})\", fontsize=16, weight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # Adjust layout to prevent titles and labels from overlapping\n",
    "    # The rect argument makes space for the suptitle\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    # Save the combined figure\n",
    "    plt.savefig(\"combined_saliency_maps.png\", dpi=300)\n",
    "    plt.close(fig)  # Close the figure to free up memory\n",
    "    print(\"Combined saliency map saved as 'combined_saliency_maps.png'\")\n",
    "else:\n",
    "    print(\"No layers were processed to generate a plot.\")\n",
    "# displot Ã¼ber kernel elemente\n",
    "\n",
    "print(tabulate(cka_comparison_table, headers=[\"layer\", \"old cka\", \"new cka\"]))\n",
    "print(tabulate(correlation_table, headers=[\"layer\", \"base corr\", \"descendant corr\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = [x[1].statistic for x in correlation_table]\n",
    "plt.plot(torch.arange(len(corrs)), corrs, label=\"base\")\n",
    "corrs = [x[2].statistic for x in correlation_table]\n",
    "plt.plot(torch.arange(len(corrs)), corrs, label=\"descendant\")\n",
    "plt.xlabel(\"layer\")\n",
    "plt.ylabel(\"pearsonr\")\n",
    "plt.title(f\"Pearsonr PC1 projection with representation norm (sample={sample_id})\")\n",
    "\n",
    "ckas = torch.tensor([t[1] for t in cka_comparison_table])\n",
    "plt.plot(torch.arange(len(corrs)), ckas, label=\"cka\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d98875",
   "metadata": {},
   "outputs": [],
   "source": [
    "for toks in top_proj_tokens:\n",
    "    print(sorted(toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "cka_comparison_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0134bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "expvar_x_1 = torch.tensor([vs[0][0] for vs in pc_stats])\n",
    "expvar_y_1 = torch.tensor([vs[1][0] for vs in pc_stats])\n",
    "ckas = torch.tensor([t[1] for t in cka_comparison_table])\n",
    "\n",
    "plt.figure(figsize=(4 * 1.6, 4))\n",
    "plt.plot(torch.arange(len(expvar_x_1)), 1 - expvar_x_1.cpu(), label=\"base_model\")\n",
    "plt.plot(torch.arange(len(expvar_y_1)), 1 - expvar_y_1.cpu(), label=\"descendant\")\n",
    "plt.plot(torch.arange(len(expvar_y_1)), 1 - expvar_y_1.cpu() - expvar_x_1, label=\"sum of pc expvars\")\n",
    "plt.plot(\n",
    "    torch.arange(len(expvar_y_1)),\n",
    "    ckas,\n",
    "    label=\"cka\",\n",
    "    # torch.arange(len(expvar_y_1)), subdf[subdf[\"sample\"] == f\"sample_{sample_id}\"][\"cka_linear\"].values, label=\"cka\"\n",
    ")\n",
    "plt.plot(\n",
    "    torch.arange(len(expvar_y_1)),\n",
    "    torch.tensor([t[2] for t in cka_comparison_table]),\n",
    "    label=\"cka w/o max pc samples\",\n",
    ")\n",
    "plt.title(f\"sample {sample_id}\")\n",
    "secax = plt.gca().secondary_yaxis(\n",
    "    \"right\",\n",
    ")\n",
    "secax.set_ylabel(\"CKA\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"1 - Explained variance by PC1 in representations\")\n",
    "plt.show()\n",
    "\n",
    "# sns.relplot(\n",
    "#     data=subdf[subdf[\"sample\"] == f\"sample_{sample_id}\"],\n",
    "#     x=\"layer\",\n",
    "#     y=\"cka_linear\",\n",
    "#     hue=\"sample\",\n",
    "#     kind=\"line\",\n",
    "#     legend=True,\n",
    "#     alpha=0.3,\n",
    "#     height=4\n",
    "# )\n",
    "\n",
    "# bathtub auch mit qwen math vs math-intruct. mit llama3.1 keine bathtub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (proj_base, proj_desc) in enumerate(projs):\n",
    "    sns.displot(\n",
    "        proj_desc.cpu(),\n",
    "        log_scale=True,\n",
    "    )\n",
    "    plt.title(f\"layer {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "expvar_x_1 = torch.tensor([vs[0][0] for vs in pc_stats])\n",
    "expvar_y_1 = torch.tensor([vs[1][0] for vs in pc_stats])\n",
    "\n",
    "plt.plot(torch.arange(len(expvar_x_1)), expvar_x_1.cpu(), label=\"base_model\")\n",
    "plt.plot(torch.arange(len(expvar_y_1)), expvar_y_1.cpu(), label=\"descendant\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99614f59",
   "metadata": {},
   "source": [
    "#### massive activation-style plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7444c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(len(h_base)):\n",
    "    # layer_idx = 7\n",
    "\n",
    "    # base model\n",
    "    # X = h_base[layer_idx].to(\"cpu\").squeeze(0).to(torch.float32)\n",
    "    # tokenizer = tokenizer_base\n",
    "\n",
    "    # descendant\n",
    "    X = h_descendant[layer_idx].to(\"cpu\").squeeze(0).to(torch.float32)\n",
    "    tokenizer = tokenizer_descendant\n",
    "\n",
    "    k = 3\n",
    "    topk = torch.linalg.norm(X, dim=1).topk(k)\n",
    "\n",
    "    fig = plt.figure(figsize=plt.figaspect(1 / k))\n",
    "    for i, idx in enumerate(topk.indices):\n",
    "        ax = fig.add_subplot(1, k, i + 1, projection=\"3d\")\n",
    "        surrounding_tokens = torch.tensor(\n",
    "            [idx.item() + i for i in range(-4, 4) if 0 < idx.item() + i < tokens.size(0)]\n",
    "        )\n",
    "\n",
    "        acts = X[surrounding_tokens].cpu()\n",
    "        acts.size()\n",
    "\n",
    "        dims = torch.arange(acts.size(1))\n",
    "\n",
    "        surrounding_tokens = surrounding_tokens.unsqueeze(0)\n",
    "        strs = [f\"{tokenizer.decode(tokens[i]).replace('\\n', '\\\\n')} ({i})\" for i in surrounding_tokens[0]]\n",
    "        dims = dims.unsqueeze(-1)\n",
    "        ax.plot_wireframe(surrounding_tokens, dims, acts.abs().T, rstride=0, color=\"royalblue\", linewidth=2.5)\n",
    "        ax.set_xticks(surrounding_tokens[0])\n",
    "        ax.set_xticklabels(strs, rotation=45, ha=\"right\")\n",
    "        ax.set_title(f\"{layer_idx=}, tok_pos={idx.item()}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b837417",
   "metadata": {},
   "source": [
    "#### stats of kernel matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cka = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cka_long = df_cka.melt(\n",
    "    [\"layer\", \"model\"],\n",
    "    [\"grad_min\", \"grad_max\", \"grad_mean\", \"grad_median\", \"kernel_min\", \"kernel_max\", \"kernel_mean\", \"kernel_median\"],\n",
    ")\n",
    "df_cka_long.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def value_to_magnitude(x: float):\n",
    "    if x > 0:\n",
    "        return math.floor(math.log10(x))\n",
    "    elif x < 0:\n",
    "        return math.floor(math.log10(-1 * x))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "df_cka_long[\"magnitude\"] = df_cka_long[\"value\"].apply(value_to_magnitude)\n",
    "sns.catplot(\n",
    "    df_cka_long, x=\"layer\", hue=\"model\", y=\"magnitude\", col=\"variable\", sharey=False, col_wrap=4, kind=\"point\"\n",
    ")\n",
    "g = sns.catplot(\n",
    "    df_cka_long,\n",
    "    x=\"layer\",\n",
    "    hue=\"model\",\n",
    "    y=\"value\",\n",
    "    col=\"variable\",\n",
    "    sharey=False,\n",
    "    col_wrap=4,\n",
    "    kind=\"point\",\n",
    "    log_scale=True,\n",
    "    height=3,\n",
    "    aspect=1.61,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gram_x_linear.grad is not None\n",
    "kernel = gram_x_linear.cpu()\n",
    "grad = gram_x_linear.grad.data.clone().detach().cpu()\n",
    "\n",
    "# print(f\"{grad.min()=}, {grad.max()=}, {grad.mean()=}, {grad.median()=}\")\n",
    "# print(f\"{kernel.min().item()=}, {kernel.max()=}, {kernel.mean()=}, {kernel.median()=}\")\n",
    "# sns.heatmap(gram_x_linear.grad.cpu().abs())\n",
    "\n",
    "with torch.no_grad():\n",
    "    downsampler = torch.nn.MaxPool2d(32)\n",
    "    downsampled = downsampler(grad.unsqueeze(0).unsqueeze(0))\n",
    "    sns.heatmap(downsampled.cpu().abs().squeeze().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef051bed",
   "metadata": {},
   "source": [
    "#### important token pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_biggest_magnitude_indices_and_values(kernel: torch.Tensor, k: int = 5, largest: bool = True):\n",
    "    flat_kernel = kernel.view(-1)\n",
    "    topk = flat_kernel.abs().topk(k=k, largest=largest)\n",
    "    rows, cols = kernel.size()\n",
    "\n",
    "    indices = []\n",
    "    for idx in topk.indices:\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        indices.append([row, col])\n",
    "    indices = torch.tensor(indices)\n",
    "\n",
    "    return indices, topk.values\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_token_pairs(\n",
    "    tokens: torch.Tensor,\n",
    "    idxs: torch.Tensor,\n",
    "    tokenizer: Callable,\n",
    "    values: None | torch.Tensor = None,\n",
    "    title: str = \"\",\n",
    "    ctx_len: int = 5,\n",
    "):\n",
    "    table = []\n",
    "    for i, (idx1, idx2) in enumerate(idxs):\n",
    "        tok1 = tokenizer.decode(tokens[idx1])\n",
    "        tok2 = tokenizer.decode(tokens[idx2])\n",
    "        ctx1 = tokenizer.decode(tokens[idx1 - ctx_len : idx1 + ctx_len])\n",
    "        ctx2 = tokenizer.decode(tokens[idx2 - ctx_len : idx2 + ctx_len])\n",
    "        val = values[i] if values is not None else None\n",
    "        table.append((tok1, tok2, val, ctx1, ctx2))\n",
    "    if title:\n",
    "        print(title)\n",
    "    print(tabulate(table, headers=[\"tok1\", \"tok2\", \"val\", \"context1\", \"context2\"]))\n",
    "    print()\n",
    "\n",
    "\n",
    "k = 10\n",
    "idxs, vals = get_biggest_magnitude_indices_and_values(gram_x_linear, k, largest=True)\n",
    "decode_token_pairs(tokens, idxs, tokenizer_base, vals, title=\"base model qwen-math-base-7b\")\n",
    "\n",
    "idxs, vals = get_biggest_magnitude_indices_and_values(gram_y_linear, k, largest=True)\n",
    "decode_token_pairs(tokens, idxs, tokenizer_descendant, vals, title=\"descendent model Qwen2.5-Math-7B-Oat-Zero\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c0eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
